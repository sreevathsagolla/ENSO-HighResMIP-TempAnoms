{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2eaaf5-3016-46b0-af35-3933da4df2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for preparing data for CMIP6 HighResMIP model simulations, NPD_eORCA025 atmosphere-forced ocean-only \n",
    "model runs, and observations-based datasets\n",
    "\n",
    "Processing in this notebook includes:\n",
    "-------------------------------------\n",
    "1. Computing con. temperature data and interpolating/remapping to a regular lat-lon grid \n",
    "over the Pacific (30˚S-30˚N; 120˚E-290˚E), using bilinear interpolation. The grid description\n",
    "for which was manually configured to be a 0.25˚ regular grid and saved as ./data/griddes_025.grd\n",
    "\n",
    "2. Calculating anomalies using centred moving 30yr baselines, for which the bounds were\n",
    "manually configured and saved as a .csv files in ./data/ (with prefix as Centred_BS_bounds),\n",
    "separetely for CMIP6 HighResMIP total period (1950-2050) and Observational datasets and NPD_eOR025 \n",
    "runs (1976-2023).\n",
    "\n",
    "3. Calculate depth of 20˚ isotherm from computed con. temp data, and saving the file.\n",
    "\n",
    "4. Compute zonal wind anomalies at 850hPa for CMIP6 HighResMIP and ERA5 Reanalysis.\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "Author: Sreevathsa G. (sg13n23@soton.ac.uk; ORCID ID: 0000-0003-4084-9677)\n",
    "Last updated: 11 December 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a43d3-419a-41ee-8c2c-2a0d88f7b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import gsw\n",
    "import glob\n",
    "import xarray as xr\n",
    "import os\n",
    "from itertools import product\n",
    "from utils import setup_dask_cluster, cdo_remapbil, z20_calculator, flatten, renamer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26995df5-65ce-4e89-a1ca-cf0eac4613a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset name lists\n",
    "PARENT_DIR = '/badc/cmip6/data/CMIP6/HighResMIP/'\n",
    "HIGHRESMIP_MODEL_PATH_SUFFIX = ['MPI-ESM1-2-HR', 'MPI-ESM1-2-XR', 'HadGEM3-GC31-HH', \n",
    "                                'EC-Earth3P-HR', 'CMCC-CM2-VHR4', 'CNRM-CM6-1-HR']\n",
    "EXP_TYPES = {'CTRL' : ['control-1950'], \n",
    "             'HIST-FUT' : ['hist-1950', 'highres-future']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505836a-9159-4ad9-a004-7f346548f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Dask client for this notebook.\n",
    "cluster, client = setup_dask_cluster()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587b39b-b6d3-4c9d-b847-2f7871e6d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATING CON. TEMPERATURE DATA OVER A PACIFIC SUBSET USING A GSW FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "# CMIP6 HighResMIP\n",
    "# ----------------\n",
    "\n",
    "for model, exp_type in tqdm(list(product(HIGHRESMIP_MODEL_PATH_SUFFIX, EXP_TYPES))):\n",
    "    # Creating a {model}_{exp_type} sub-directory if it doesn't exist\n",
    "    if os.path.exists(f'./data/thetao_con/{model}_{exp_type}/') == False:\n",
    "        os.mkdir(f'./data/thetao_con/{model}_{exp_type}/')\n",
    "    for year in tqdm(np.arange(1950, 2050+1), leave = False):\n",
    "        # Filenames corresponding to each year's data: pt = potential temperature (var_name : thetao, in degC) and so = absolute salinity (var_name : so, in g/kg)\n",
    "        fnames_pt = flatten([sorted(glob.glob(PARENT_DIR + f'*/{model}/{exp_id}/r1*/Omon/thetao/*/latest/*{year}??.nc')) for exp_id in EXP_TYPES[exp_type]])\n",
    "        fnames_so = flatten([sorted(glob.glob(PARENT_DIR + f'*/{model}/{exp_id}/r1*/Omon/so/*/latest/*{year}??.nc')) for exp_id in EXP_TYPES[exp_type]])\n",
    "        # Ensuring the lists are not empty i.e., files for the iterated year exists\n",
    "        if (fnames_pt != []) & (fnames_so != []):\n",
    "            # Reading and bilinearly interpolating the data into a 0.25deg regular lat-lon grid\n",
    "            ds_thetao = cdo_remapbil(filename_list = fnames_pt, var_name = 'thetao')\n",
    "            ds_so = cdo_remapbil(filename_list = fnames_so, var_name = 'so')\n",
    "            # Computing conservative temperature from potential temperature and salinity using gsw.conversions.CT_from_pt function\n",
    "            ds = gsw.conversions.CT_from_pt(SA = ds_so['so'], pt = ds_thetao['thetao']).rename('thetao_con').to_dataset()\n",
    "            # Saving the file as a single-year file\n",
    "            ds.to_netcdf(f'./data/thetao_con/{model}_{exp_type}/BILINTERP_PAC_THETAO_CON_CMIP6_HIGHRESMIP_{model}_{exp_type}_{year}.nc')\n",
    "    # Cleaning-up cdo tempdir\n",
    "    cdo.cleanTempDir()\n",
    "    \n",
    "# Observations-based Products\n",
    "# ---------------------------\n",
    "\n",
    "for obs_dataset in ['ORAS5', 'EN4']:\n",
    "    # The original files are saved at per-month frequency, so looping through each monthly timestep from 1976-2023\n",
    "    for ts in tqdm(pd.date_range(start = '1976-01-01', end = '2023-12-31', freq = '1MS')):\n",
    "        # Creating a {obs_dataset} sub-directory if it doesn't exist\n",
    "        if os.path.exists(f'./data/thetao_con/{obs_dataset}/') == False:\n",
    "            os.mkdir(f'./data/thetao_con/{obs_dataset}/')\n",
    "\n",
    "        # Filenames corresponding to each year's data: pt = potential temperatur and so = absolute salinity, varname_pt/so set accordingly\n",
    "        if obs_dataset == 'ORAS5':\n",
    "            f_so = glob.glob(f'/dssgfs01/working/sg13n23/OBS_DATA/ORAS5/orig_data/vosaline*{ts.year}{ts.month:02d}*')[0]\n",
    "            f_pt = glob.glob(f'/dssgfs01/working/sg13n23/OBS_DATA/ORAS5/orig_data/votemper*{ts.year}{ts.month:02d}*')[0]\n",
    "            varname_pt = 'votemper'\n",
    "            varname_so = 'vosaline'\n",
    "        elif obs_dataset == 'EN4':\n",
    "            f_so = f_pt = f'/dssgfs01/working/sg13n23/OBS_DATA/EN4/orig_data/EN.4.2.2.f.analysis.g10.{ts.year}{ts.month:02d}.nc'\n",
    "            varname_pt = 'temperature'\n",
    "            varname_so = 'salinity'\n",
    "            \n",
    "        # Reading and bilinearly interpolating the data into a 0.25deg regular lat-lon grid\n",
    "        ds_thetao = cdo_remapbil(filename_list = [f_pt], var_name = varname_pt)\n",
    "        ds_so = cdo_remapbil(filename_list = [f_so], var_name = varname_so)\n",
    "        # Computing con. temperature from potential temperature and salinity using gsw.conversions.CT_from_pt function\n",
    "        ds = gsw.conversions.CT_from_pt(SA = ds_so[varname_so], pt = ds_thetao[varname_pt]).rename('thetao_con').to_dataset()\n",
    "        # Setting time axis so that it's the start of the month\n",
    "        ds['time'] = pd.to_datetime([str(_)[0:8]+'01' for _ in ds['time'].values])\n",
    "        # Saving the file as a single-year file\n",
    "        ds.to_netcdf(f'./data/thetao_con/{obs_dataset}/BILINTERP_PAC_THETAO_CON_{obs_dataset}_{ts.year}{ts.month:02d}.nc')\n",
    "        # Cleaning-up cdo tempdir\n",
    "        cdo.cleanTempDir()  \n",
    "\n",
    "# NPD_eORCA025 experiments \n",
    "# -------------------------\n",
    "# NOTE: thetao_con is pre-calculated online when the model is run, so just bilinearly interpolating and saving data here\n",
    "\n",
    "for npd_exp in ['NPD_eORCA025_ERA5', 'NPD_eORCA025_JRA55']:\n",
    "    # The original files are saved at per-month frequency, so looping through each monthly timestep from 1976-2023\n",
    "    for ts in tqdm(pd.date_range(start = '1976-01-01', end = '2023-12-31', freq = '1MS')):\n",
    "        # Reading and bilinearly interpolating the data into a 0.25deg regular lat-lon grid\n",
    "        fname = f'/dssgfs01/scratch/npd/simulations/{npd_exp[4:]}/{ts.year}/eORCA025_1m_grid_T_{ts.year}{ts.month:02d}-{ts.year}{ts.month:02d}.nc'\n",
    "        ds = cdo_remapbil(filename_list = [fname], var_name = 'thetao_con')['thetao_con'].to_dataset()\n",
    "        # Setting time axis appropriately\n",
    "        ds['time'] = pd.to_datetime([str(_)[0:8]+'01' for _ in ds['time'].values])\n",
    "        # Setting time axis so that it's the start of the month\n",
    "        ds.to_netcdf(f'./data/thetao_con/{npd_exp}/BILINTERP_PAC_THETAO_CON_{npd_exp}_{ts.year}{ts.month:02d}.nc')\n",
    "        # Cleaning-up cdo tempdir\n",
    "        cdo.cleanTempDir() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea02eb4-abfd-458e-b87c-bc4c76a66ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATING MOVING-BASELINE ANOMALIES FOR EACH DATASET OVER THE PACIFIC\n",
    "# =============================================================================\n",
    "\n",
    "# CMIP6 HighResMIP\n",
    "# ----------------\n",
    "\n",
    "# Reading in the CSV file containing the 30-year climatological (centred) baseline bounds for consecutive 5 years in 1950-2050 (file prepared manually)\n",
    "bounds_df = pd.read_csv('./data/Centred_BS_bounds_CMIP6_HIGHRESMIP_1950-2050.csv')\n",
    "for model, exp_type in tqdm(list(product(HIGHRESMIP_MODEL_PATH_SUFFIX, EXP_TYPES))):\n",
    "    # Creating a {model}_{exp_type} directory if it doesn't exist\n",
    "    if os.path.exists(f'./data/anomalies/{model}_{exp_type}/') == False:\n",
    "        os.mkdir(f'./data/anomalies/{model}_{exp_type}/')\n",
    "\n",
    "    # Lazy-loading (Dask) in all the con. temp. data (thetao_con) processed and saved above, using open_mfdataset\n",
    "    ds = xr.open_mfdataset(f'./data/thetao_con/{model}_{exp_type}/BILINTERP_PAC_THETAO_CON_CMIP6_HIGHRESMIP_{model}_{exp_type}_????.nc')\n",
    "    # Setting time axis so that it's the start of the month, and is of datetime64[ns] type\n",
    "    ds['time'] = pd.date_range(start = '1950-01-01', periods = ds.time.shape[0], freq = '1MS')\n",
    "\n",
    "    # Looping through each year from 1950 to 2050, to calculate each year's anomalies\n",
    "    for year in tqdm(np.arange(1950, 2050+1), leave = False):\n",
    "        # Subsetting the row in bounds_df dataframe, where 'year' lies in (START_Y, END_Y) (i.e., the 5-year period), to determine the baseline bounds to use\n",
    "        bs_df = bounds_df[(year>=bounds_df['START_Y']) & (year<=bounds_df['END_Y'])]\n",
    "        bs_start, bs_end = str(bs_df['BS_START'].item()), str(bs_df['BS_END'].item())\n",
    "\n",
    "        # Calculating monthly mean climatological baseline, corresponding to the 5-year period where 'year' lies in\n",
    "        baseline = ds.sel(time = slice(bs_start, bs_end)).groupby('time.month').mean()\n",
    "        # Calculating the temperature anomalies, and renaming the variable appropriately\n",
    "        anomalies = ds.sel(time = str(year)).groupby('time.month') - baseline\n",
    "        anomalies = anomalies.rename({'thetao_con':'anomaly'}).drop_vars('month')\n",
    "        # Setting attribute information according to the processing done\n",
    "        anomalies.attrs['standard_name'] = 'anomaly_sea_water_conservative_temperature'\n",
    "        anomalies.attrs['long_name'] = f'Anomaly of Sea Water Conservative Temperature with baseline {bs_start}-{bs_end}'\n",
    "        anomalies.attrs['units'] = 'degC'\n",
    "        # Saving the processed anomaly data\n",
    "        anomalies.to_netcdf(f'./data/anomalies/{model}_{exp_type}/BILINTERP_PAC_ANOM_BS{bs_start}-{bs_end}_CMIP6_HIGHRESMIP_{model}_{exp_type}_{year}.nc')\n",
    "\n",
    "# Observations-based Products and NPD_eORCA025 exps.\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Reading in the CSV file containing the 30-year climatological (centred) baseline bounds for consecutive 5 years in 1976-2023 recent past period (file prepared manually)\n",
    "bounds_df = pd.read_csv('./data/Centred_BS_bounds_NPD_eORCA025_OBS_1976-2023.csv')\n",
    "for dataset in ['ORAS5', 'EN4', 'NPD_eORCA025_ERA5', 'NPD_eORCA025_JRA55']:\n",
    "    # Creating a {dataset} sub-directory if it doesn't exist\n",
    "    if os.path.exists(f'./data/anomalies/{dataset}/') == False:\n",
    "            os.mkdir(f'./data/anomalies/{dataset}/')\n",
    "    # Looping through each monthly timestamp, as done previously while preparing and saving thetao_con for observations and NPD experiments\n",
    "    for ts in tqdm(pd.date_range(start = '1976-01-01', end = '2023-12-31', freq = '1MS')):\n",
    "        # Subsetting the row in bounds_df dataframe, where 'year' lies in (START_Y, END_Y) (i.e., the 5-year period), to determine the baseline bounds to use\n",
    "        bs_df = bounds_df[(ts.year>=bounds_df['START_Y']) & (ts.year<=bounds_df['END_Y'])]\n",
    "        bs_start, bs_end = bs_df['BS_START'].item(), bs_df['BS_END'].item()\n",
    "\n",
    "        # Making a list of files to load to compute the climatological baseline corresponding to 'ts.month'\n",
    "        bs_fnames = []\n",
    "        for _ in range(bs_start, bs_end+1):\n",
    "            bs_fnames += [f'./data/thetao_con/{dataset}/BILINTERP_PAC_THETAO_CON_{dataset}_{_}{ts.month:02d}.nc']\n",
    "            \n",
    "        # Loading in the data file for 'ts', computing climatological baseline mean and computing anomalies\n",
    "        ds = xr.open_dataset(f'./data/thetao_con/{dataset}/BILINTERP_PAC_THETAO_CON_{dataset}_{ts.year}{ts.month:02d}.nc')\n",
    "        baseline = xr.open_mfdataset(bs_fnames).groupby('time.month').mean()\n",
    "        anomalies = ds.groupby('time.month') - baseline\n",
    "        # Renaming the variable appropriately\n",
    "        anomalies = anomalies.rename({'thetao_con':'anomaly'}).drop_vars('month')\n",
    "        # Setting attribute information according to the processing done\n",
    "        anomalies.attrs['standard_name'] = 'anomaly_sea_water_conservative_temperature'\n",
    "        anomalies.attrs['long_name'] = f'Anomaly of Sea Water Conservative Temperature with baseline {bs_start}-{bs_end}'\n",
    "        anomalies.attrs['units'] = 'degC'\n",
    "        # Saving the processed anomaly data\n",
    "        anomalies.to_netcdf(f'./data/anomalies/{dataset}/BILINTERP_PAC_ANOM_BS{bs_start}-{bs_end}_{dataset}_{ts.year}{ts.month:02d}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b41de-a669-4bc4-a5f0-632d526ef1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATING DEPTH OF 20°C (Z20) FOR EACH MODEL/OBSERVATIONAL DATASET\n",
    "# =============================================================================\n",
    "\n",
    "# CMIP6 HighResMIP\n",
    "# ----------------\n",
    "\n",
    "for model, exp_type in tqdm(list(product(HIGHRESMIP_MODEL_PATH_SUFFIX, EXP_TYPES))):\n",
    "    ds_z20 = [] # Empty list to save computed Z20 for each year\n",
    "    # Looping through each year from 1950 to 2050\n",
    "    for year in tqdm(np.arange(1950, 2050+1), leave = False):\n",
    "        # Loading in the data and ensuring that the time axis is the start of the month, and is of datetime64[ns] type\n",
    "        ds = xr.open_dataset(f'./data/thetao_con/{model}_{exp_type}/BILINTERP_PAC_THETAO_CON_CMIP6_HIGHRESMIP_{model}_{exp_type}_{year}.nc')\n",
    "        ds['time'] = pd.date_range(start = f'{year}-01-01', periods = ds.time.shape[0], freq = '1MS')\n",
    "        # Cropping to 5˚S - 5˚N\n",
    "        ds = ds.sel(lat = slice(-5,5), lon = slice(140,280))\n",
    "        # Calculating Z20 depths and appending to the ds_z20 list\n",
    "        ds_z20 += [z20_calculator(ds)]\n",
    "    # Merging calculated Z20 for all years to a single dataset\n",
    "    ds_z20 = xr.merge(ds_z20)\n",
    "    # Interpolating to a finer longitude grid, averaging across 'lat' dimension and expanding dims to include 'model' name information\n",
    "    ds_z20 = ds_z20.interp(lon = np.arange(140,280.01, 0.1)).mean(dim = ['lat']).expand_dims(dataset = [model])\n",
    "    # Saving the processed Z20 data\n",
    "    ds_z20.to_netcdf(f'./data/z20_depths/Z20_DEPTHS_{model}_{exp_type}_1950-2050.nc')\n",
    "\n",
    "# Observations-based Products and NPD_eORCA025 exps.\n",
    "# --------------------------------------------------\n",
    "\n",
    "for dataset in ['ORAS5', 'EN4', 'NPD_eORCA025_ERA5', 'NPD_eORCA025_JRA55']:\n",
    "    ds_z20 = [] # Empty list to save computed Z20 for each monthly timestep\n",
    "    # Looping through each monthly timestep (as files are saved as such) from 1976 to 2023 of the recent past period\n",
    "    for ts in tqdm(pd.date_range(start = '1976-01-01', end = '2023-12-31', freq = '1MS')):\n",
    "        # Loading in the data and ensuring that the time axis is the start of the month, and is of datetime64[ns] type\n",
    "        ds = xr.open_dataset(f'./data/thetao_con/{dataset}/BILINTERP_PAC_THETAO_CON_{dataset}_{ts.year}{ts.month:02d}.nc')\n",
    "        ds['time'] = pd.date_range(start = f'{year}-01-01', periods = ds.time.shape[0], freq = '1MS')\n",
    "        # Cropping to 5˚S - 5˚N\n",
    "        ds = ds.sel(lat = slice(-5,5), lon = slice(140,280))\n",
    "        # Calculating Z20 depths and appending to the ds_z20 list\n",
    "        ds_z20 += [z20_calculator(ds)]\n",
    "    # Merging calculated Z20 for all monthly timesteps to a single dataset\n",
    "    ds_z20 = xr.merge(ds_z20)\n",
    "    # Interpolating to a finer longitude grid, averaging across 'lat' dimension and expanding dims to include 'dataset' name information\n",
    "    ds_z20 = ds_z20.interp(lon = np.arange(140,280.01, 0.1)).mean(dim = ['lat']).expand_dims(dataset = [dataset])\n",
    "    # Saving the processed Z20 data\n",
    "    ds_z20.to_netcdf(f'./data/z20_depths/Z20_DEPTHS_{dataset}_1950-2050.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fda7f-eddf-4576-bf5e-7ff6364325b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CALCULATING ZONAL WIND ANOMALIES FOR CMIP6 HIGHRESMIP AND ERA5 REANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# CMIP6 HighResMIP\n",
    "# ----------------\n",
    "\n",
    "for model, exp_type in tqdm(list(product(HIGHRESMIP_MODEL_PATH_SUFFIX, EXP_TYPES))):\n",
    "    # Zonal Wind @ 850 hPa - Bilinearly interpolated to a subset over the Pacific\n",
    "    # ---------------------------------------------------------------------------\n",
    "    \n",
    "    # Reading and bilinearly interpolating the data into a 0.25deg regular lat-lon grid, selecting a single pressure level of interest (argument passed in hPa)\n",
    "    fnames_ua = flatten([sorted(glob.glob(PARENT_DIR + f'*/{model}/{exp_id}/r1*/Amon/ua/*/latest/*.nc')) for exp_id in EXP_TYPES[exp_type]])\n",
    "    ds = cdo_remapbil(filename_list = fnames_ua, var_name = 'ua', plev = 850).squeeze()\n",
    "    # Ensuring that the time axis is the start of the month, and is of datetime64[ns] type\n",
    "    ds['time'] = pd.date_range(start = '1950-01-01', periods = ds.time.shape[0], freq = '1MS')\n",
    "    # Saving the interpolated/cropped zonal wind data @ 850 hPa\n",
    "    ds.to_netcdf(f'./data/zon_wnds_850hpa/BILINTERP_PAC_ZON_WND_850HPA_{model}_{exp_type}.nc')\n",
    "    # Cleaning-up cdo tempdir\n",
    "    cdo.cleanTempDir()\n",
    "    \n",
    "    # Calculating zonal wind anomalies @ 850 hPa\n",
    "    # ------------------------------------------\n",
    "    \n",
    "    # Compute anomalies separately for each forcing group, using a full-period\n",
    "    # climatology specific to each sub-period:\n",
    "    #   - ctrl-1950      : mean over 1950–2050\n",
    "    #   - hist-1950      : mean over 1950–2014\n",
    "    #   - highres-future : mean over 2015–2050\n",
    "    # In other words, each experiment's anomalies are computed relative to its\n",
    "    # own full available climatology, not a shared baseline across groups.\n",
    "\n",
    "    if exp_type == 'CTRL':\n",
    "        ds_anomalies = ds['ua'].groupby('time.month') - ds['ua'].groupby('time.month').mean()\n",
    "    elif exp_type == 'HIST-FUT':\n",
    "        ds_anomalies = xr.concat([ds['ua'].sel(time = slice('1950', '2014')).groupby('time.month') \n",
    "                                  - ds['ua'].sel(time = slice('1950', '2014')).groupby('time.month').mean(),\n",
    "                                  \n",
    "                                  ds['ua'].sel(time = slice('2015', '2050')).groupby('time.month') \n",
    "                                  - ds['ua'].sel(time = slice('2015', '2050')).groupby('time.month').mean()],\n",
    "                                dim = 'time')\n",
    "    \n",
    "    # Saving information on the climatological periods used to calculate anomalies in attrs.history, for future reference\n",
    "    ds_anomalies.attrs['history'] = 'Zonal wind anomalies calculated with respect to climatological mean over periods: 1950-2050 (control-1950), 1950-2014 (hist-1950) and 2015-2050 (highres-future).'\n",
    "    # Saving the processed zonal wind anomalies @ 850 hPa\n",
    "    ds_anomalies.to_netcdf(f'./data/zon_wnds_850hpa/BILINTERP_PAC_ZON_WND_ANOM_850HPA_{model}_{exp_type}.nc')\n",
    "\n",
    "# ERA5 Reanalysis\n",
    "# ----------------\n",
    "# Reading and bilinearly interpolating the data into a 0.25deg regular lat-lon grid\n",
    "ds_era5 = cdo.remapbil('./data/griddes_025.grd', input='./data/zon_wnds_850hpa/UV_WND_850HPA_ERA5_REANALYSIS.nc', returnXDataset = True)\n",
    "# Saving the cropped/interpolated zonal wind @ 850 hPa data\n",
    "ds_era5.to_netcdf('./data/zon_wnds_850hpa/BILINTERP_PAC_UV_WND_850HPA_ERA5_REANALYSIS.nc')\n",
    "\n",
    "# Selecting data for only 1976-2023 recent past period, renaming some variables, and squeezing 1-value dimensions\n",
    "ds_era5 = ds_era5.rename({'valid_time': 'time', 'u':'ua', 'v':'va'}).sel(time = slice('1976', '2023')).squeeze()\n",
    "# calculating zonal wind anomalies using the full 1976-2023 period as the climatological baseline\n",
    "ds_era5_anoms = ds_era5.groupby('time.month') - ds_era5.groupby('time.month').mean()\n",
    "# Saving the processed zonal wind anomalies @ 850 hPa\n",
    "ds_era5_anoms.to_netcdf('./data/zon_wnds_850hpa/BILINTERP_PAC_UV_WND_ANOM_850HPA_ERA5_REANALYSIS_1976-2023.nc')\n",
    "\n",
    "# NOTE: The file ./data/zon_wnds_850hpa/UV_WND_850HPA_ERA5_REANALYSIS.nc was obtained directly from the Copernicus Climate Data Store \n",
    "# (ERA5, 850 hPa u and v winds). The two wind components were downloaded and merged into a single NetCDF file externally; this step is \n",
    "# not part of the preprocessing pipeline implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46217531-df91-4cdb-b343-5526f221013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db42e37-4c88-4615-b417-317a32b0d6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
